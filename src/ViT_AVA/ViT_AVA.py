# -*- coding: utf-8 -*-
"""pruebas-ava.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pVXozWgfbcnmS8gmtHIU_QWHwWe6aiyk

**Distribuci칩n**: tuplas de la probabilidad de que sea buena y mala [[0.1,0.1,0.1,...(10 veces)],...] losses:EMD; 10 neuronas output con softmax
>(distribution, none)


**Clases (probabilidad)**: [[0.34,0.66],...] losses: categoricalCrossEntropy; 2 neuronas output con softmax
>(mean, binaryWeights)


**Regresi칩n**: etiquetas entre 0-1 con la puntuacion media de que sea buena la foto [0.5,0.3,0.24,...] losses:MSE; 1 neurona output sin func activacion
>(mean, none)


**Clases (etiqueta)**: etiquetas o 1 o 0 si las fotos son buenas o malas [0,1,0,1,0,0,0,1] losses:binaryCrossEntropy; 1 neurona output con softmax
>(mean, binaryClasses)
"""

import sys
sys.path.insert(1, 'AQA-framework-dev_coteach')

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_addons as tfa
import pickle

import tfimm
from datasets import AVA_generators
from tensorflow.keras.losses import MSE, MAE

model_list = tfimm.list_models(pretrained="timm")
str_match = [s for s in model_list if "vit" in s]
str_match

vit_model = tfimm.create_model("vit_base_patch32_224", pretrained="timm")
vit_model.summary()

"""image size del ViT"""

image_size = 224

"""input shape de AVA"""

input_shape = (224, 224, 3)

avaRegression = AVA_generators()

def parse_image(filename, label):
  image = tf.io.read_file(filename)
  image = tf.io.decode_jpeg(image)
  image = tf.image.convert_image_dtype(image, tf.float32)
  image = tf.image.resize(image, [image_size, image_size])
  return image, label

preprocess_func = tfimm.create_preprocessing("vit_base_patch32_224", dtype="float32")
preprocess = lambda img, lab: (preprocess_func(img), lab)

x_train = tf.data.Dataset.from_tensor_slices((avaRegression.train_image_paths, avaRegression.train_scores)).map(parse_image).map(preprocess).shuffle(1024).batch(64).prefetch(-1)
x_test = tf.data.Dataset.from_tensor_slices((avaRegression.test_image_paths, avaRegression.test_scores)).map(parse_image).map(preprocess).batch(64).prefetch(-1)
x_val = tf.data.Dataset.from_tensor_slices((avaRegression.val_image_paths, avaRegression.val_scores)).map(parse_image).map(preprocess).batch(64).prefetch(-1)

# data_augmentation = keras.Sequential(
#     [
#         layers.Normalization(),
#         layers.Resizing(image_size, image_size),
#         layers.RandomFlip("horizontal"),
#         layers.RandomRotation(factor=0.02),
#         layers.RandomZoom(
#             height_factor=0.2, width_factor=0.2
#         )
#     ],
#     name="data_augmentation",
# )

"""a침adimos las capas de entrada y salida correspondiente a nuestro problema (AVA)"""

inputs = layers.Input(shape=input_shape)
# augmented = data_augmentation(inputs)
features = vit_model(inputs)
logits = keras.layers.Dense(1)(features)
model = keras.Model(inputs=inputs, outputs=logits)
model.summary()

"""par치metros para compilar, entrenar y evaluar el modelo

a ajustar posteriormente
"""

learning_rate = 0.000001
weight_decay = 0.0001
num_epochs = 30

def run_experiment(model):
    optimizer = tfa.optimizers.AdamW(
        learning_rate=learning_rate, weight_decay=weight_decay
    )

    model.compile(
        optimizer=optimizer,
        loss=MSE,
        metrics=[
            MSE,
            MAE
        ]
    )

    checkpoint_filepath = "checkpoint_vit/checkpoint"
    checkpoint_callback = keras.callbacks.ModelCheckpoint(
        checkpoint_filepath,
        monitor="val_mean_squared_error",
        save_best_only=True,
        save_weights_only=True,
    )

    history = model.fit(
        x= x_train,
        epochs=num_epochs,
        validation_data=x_val,
        callbacks=[checkpoint_callback],
    )

    model.load_weights(checkpoint_filepath)
    results = model.evaluate(x_test)
    print("test loss, test MSE, test MAE:", results)

    return history.history

history = run_experiment(model)

with open('outputs_vit_ava.pickle', 'wb') as f:
    pickle.dump(history, f)
